{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCDRD4_WHwIE"
   },
   "source": [
    "# Tensorflow 2.0 Matrix Factorization for prediction of movie ratings using tf.feature_column and tf.Estimator API\n",
    "Dataset:https://grouplens.org/datasets/movielens/1m/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vkdbOVyHoJP"
   },
   "source": [
    "### Import Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "-1fJZMolCLZB",
    "outputId": "11e40366-6ac4-478e-f233-7bb25d14eacc"
   },
   "outputs": [],
   "source": [
    "# Use Tensorflow 2.0 \n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "  # Load the TensorBoard extension\n",
    "  %load_ext tensorboard\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "OSPOpE18DNLs",
    "outputId": "04737a7b-8d47-4a1f-8108-dc8d243aa0e0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "keras = tf.keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "poJ2TdqRH5DB"
   },
   "source": [
    "### Load Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "FarEB_qoDXog",
    "outputId": "6daf93da-bd6e-49bd-fb49-48cc9be2e888"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "WURq9xFjEPOa",
    "outputId": "41f266c3-517e-4fa9-ec82-8f6438e0fa9a"
   },
   "outputs": [],
   "source": [
    "!mkdir -vp data\n",
    "!cp -r '/content/drive/My Drive/collab_data/ml-1m' './data'\n",
    "!ls data/ml-1m\n",
    "\n",
    "# If dataset is big, just make a symbolic link (This doesn't make data loading any faster, but prevents possible issues with whe white space in the path)\n",
    "# !ln -s '/content/drive/My Drive/collab_data/ml-1m' './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjWt4q_Ea30B"
   },
   "source": [
    "## Data Preprocessing / Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "8SkjANrsNhnS",
    "outputId": "78c1de6e-d8c8-4559-a3f5-ea1d472f368f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_path = 'data/ml-1m'\n",
    "\n",
    "user_columns = 'UserID::Gender::Age::Occupation::Zip-code'.split('::')\n",
    "movie_columns = 'MovieID::Title::Genres'.split('::')\n",
    "ratings_columns = 'UserID::MovieID::Rating::Timestamp'.split('::')\n",
    "\n",
    "ratings_path = os.path.join(dataset_path, 'ratings.dat')\n",
    "\n",
    "df_movies = pd.read_csv(os.path.join(dataset_path, 'movies.dat'), sep='::', header=None)\n",
    "df_ratings = pd.read_csv(ratings_path, sep='::', names=ratings_columns)\n",
    "df_users = pd.read_csv(os.path.join(dataset_path, 'users.dat'), sep='::', header=None)\n",
    "\n",
    "df_ratings['UserID'] = df_ratings['UserID'] -1\n",
    "df_ratings['MovieID'] = df_ratings['MovieID'] -1\n",
    "\n",
    "df_ratings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3MzYc8RsjGF"
   },
   "outputs": [],
   "source": [
    "# | drop the Timestamp column, as we don't need that for our recommender system\n",
    "cleaned_ratings_path = os.path.join(dataset_path, 'ratings-cleaned.dat')\n",
    "df_ratings.to_csv(cleaned_ratings_path, sep=';', index=False, header=False)\n",
    "# df_ratings.drop('Timestamp', axis=1).to_csv(cleaned_ratings_path, sep=';', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "ZRS85Dhad6Mi",
    "outputId": "778b397d-b404-4a36-c308-2678af5678e7"
   },
   "outputs": [],
   "source": [
    "# | check if there are duplicates i.e. if a user rated the same item twice\n",
    "df_ratings_copy = df_ratings.copy()\n",
    "df_ratings_copy = df_ratings_copy.drop('Timestamp', axis=1)\n",
    "len(df_ratings_copy) - len(df_ratings_copy.drop_duplicates())\n",
    "\n",
    "# | check number of unique users & movies\n",
    "nr_users = len(df_ratings['UserID'].unique())\n",
    "nr_movies = len(df_ratings['MovieID'].unique())\n",
    "\n",
    "print('#Users: {}, #Movies: {}'.format(nr_users, nr_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PrcLQjGbcLY3"
   },
   "source": [
    "### A) Create dataset using CsvDataset()\n",
    "Wont work out of the box due to the '::' separator. Only slingle char separators are supported!<br>\n",
    "Another issue is that the column names are lost, i.e. the features won't be read as a dictionary {col_name: value}, which is e.g. needed if using tf.feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPM2JOXwdId9"
   },
   "outputs": [],
   "source": [
    "# dataset = tf.data.experimental.CsvDataset(cleaned_ratings_path, [tf.constant([], dtype=tf.int32)]*3, field_delim=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxITacHPc6Tv"
   },
   "source": [
    "### B) Create dataset manually using TextLineDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "_l82mVDhN10p",
    "outputId": "e465cf26-6003-490e-8f75-e3fdd566e2ad"
   },
   "outputs": [],
   "source": [
    "NR_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "ratings_columns = 'UserID::MovieID::Rating'.split('::')\n",
    "\n",
    "\n",
    "def input_fn(file_path, buffer_size=10000, nr_epochs=10, batch_size=32):\n",
    "  def preprocess(line):\n",
    "    # line = tf.strings.regex_replace(line, \"::\", \";\") # note: this function cannot be converted into a graph... will be slow\n",
    "    fields = tf.io.decode_csv(line, record_defaults=[tf.constant([], dtype=tf.int32)]*3, field_delim=\";\", select_cols=[0,1,2])\n",
    "    \n",
    "    # features = tf.stack(fields[:-1]) # decode_csv() function returns a list of scalar tensors (one per column) but we need to return 1D tensor arrays\n",
    "    label = tf.stack(fields[-1])\n",
    "    label = tf.dtypes.cast(label, tf.dtypes.int32)\n",
    "    features = dict(zip(ratings_columns[:-1], fields[:-1]))\n",
    "    \n",
    "    return features, label \n",
    "  \n",
    "  dataset = tf.data.TextLineDataset(file_path)\n",
    "  dataset = dataset.map(preprocess, num_parallel_calls=4)\n",
    "  dataset = dataset.cache()\n",
    "  dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=buffer_size, count=NR_EPOCHS))\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(1)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "\n",
    "def predict_input_fn(file_path, buffer_size=10000, batch_size=1):\n",
    "  def preprocess(line):\n",
    "    # line = tf.strings.regex_replace(line, \"::\", \";\")\n",
    "    fields = tf.io.decode_csv(line, record_defaults=[tf.constant([], dtype=tf.int32)]*3, field_delim=\";\", select_cols=[0,1,2])\n",
    "\n",
    "    features = dict(zip(ratings_columns[:-1], fields[:-1]))\n",
    "    \n",
    "    return features \n",
    "  \n",
    "  dataset = tf.data.TextLineDataset(file_path)\n",
    "  dataset = dataset.map(preprocess, num_parallel_calls=4)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "dataset = input_fn(cleaned_ratings_path, 10000, NR_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "colab_type": "code",
    "id": "Zf3MCmSPVCA3",
    "outputId": "aa95f204-524b-4428-d72e-3d601f1db15c"
   },
   "outputs": [],
   "source": [
    "for features, label in dataset.take(1):\n",
    "  print(features)  \n",
    "  print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ub0Ebnl9qlX"
   },
   "source": [
    "### Define the feature columns\n",
    "The probability of a collision if there are k categories is approximately equal to: 1 - exp(-k*(k-1)/2/hash_bucket_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "oJRrLS7CBjWG",
    "outputId": "b7a79494-1102-4d58-b040-d52f98251a63"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "def calculate_collision_probability(nr_categories, hash_bucket_size):\n",
    "  # | caluclates probability of event where at least one collision occurs\n",
    "  return 1 - math.exp(-nr_categories*(nr_categories-1)/(2*hash_bucket_size)) \n",
    "\n",
    "\n",
    "def estimate_number_of_collisions(nr_categories, hash_bucket_size): \n",
    "  return nr_categories - hash_bucket_size * (1 - ((hash_bucket_size-1)/hash_bucket_size)**nr_categories)\n",
    "\n",
    "calculate_collision_probability(nr_users, 100000)\n",
    "estimate_number_of_collisions(nr_users, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Fuyqqfl1NzXt",
    "outputId": "d1163a04-b08f-4356-cb94-47e04045094f"
   },
   "outputs": [],
   "source": [
    "print(df_ratings['UserID'].max(), df_ratings['MovieID'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYvcZAhPXV-e"
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# cat_col = tf.feature_column.categorical_column_with_hash_bucket(key=key, hash_bucket_size=1000) # currently there seems to be a bug with categorical_column_with_hash_bucket()\n",
    "\n",
    "# user_id = tf.feature_column.categorical_column_with_identity('UserID', num_buckets=6038)\n",
    "# movie_id = tf.feature_column.categorical_column_with_identity('MovieID', num_buckets=3951)\n",
    "user_id = tf.feature_column.categorical_column_with_hash_bucket('UserID', hash_bucket_size=1000, dtype=tf.dtypes.int32)\n",
    "movie_id = tf.feature_column.categorical_column_with_hash_bucket('MovieID', hash_bucket_size=1000, dtype=tf.dtypes.int32)\n",
    "\n",
    "feature_columns.append(tf.feature_column.embedding_column(user_id, 50))\n",
    "feature_columns.append(tf.feature_column.embedding_column(movie_id, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "BMYv_KW8Ir_p",
    "outputId": "0d01e87b-06be-4927-8c75-9fafa12edc45"
   },
   "outputs": [],
   "source": [
    "# | check outputs of feature layer\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "for f, l in dataset.take(1):\n",
    "  print(f)\n",
    "\n",
    "out = feature_layer(f)\n",
    "user_id_tensor, movie_id_tensor = tf.split(out, 2, axis=1)\n",
    "p = tf.reduce_sum(tf.multiply(user_id_tensor, movie_id_tensor), axis=1)\n",
    "tf.keras.losses.MSE(l, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "sQQ4k7QZoU60",
    "outputId": "af97e1c6-cf89-4bd7-9ad1-437268f9a2be"
   },
   "outputs": [],
   "source": [
    "# Caution tf.keras.losses.MSE() is not the same as tf.keras.losses.MeanSquaredError() (althouc docs lists it as alias)\n",
    "\n",
    "import inspect\n",
    "print(inspect.signature(tf.keras.losses.MeanSquaredError()))\n",
    "print(inspect.signature(tf.keras.losses.MSE)) # note: MSE without ()\n",
    "print(inspect.signature(tf.metrics.RootMeanSquaredError)) # note: MSE without ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24pk0Ivp8Rcm"
   },
   "source": [
    "### Define the model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "N4LnKEEAgx3e",
    "outputId": "97d34752-c894-4f97-e2a7-94c04e2d719e"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, feature_columns):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.embedding_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, x):\n",
    "    feature_tensor = self.embedding_layer(x)\n",
    "    \n",
    "    # | Split into two equal parts, as hash_bucket_size of both feature_columns is equal\n",
    "    user_id_tensor, movie_id_tensor = tf.split(feature_tensor, num_or_size_splits=2, axis=1)\n",
    "    \n",
    "    # | dot product of user vector with movie vector -> the more correlation between these vectors, the higher the ranking should be (label)\n",
    "    predicted_ranking = tf.reduce_sum(tf.multiply(user_id_tensor, movie_id_tensor), axis=1)\n",
    "\n",
    "    return predicted_ranking\n",
    "                \n",
    "\n",
    "def my_model_fn(\n",
    "   features, # This is batch_features from input_fn\n",
    "   labels,   # This is batch_labels from input_fn\n",
    "   mode,     # An instance of tf.estimator.ModeKeys (tf.estimator.ModeKeys.TRAIN, EVAL, PREDICT)\n",
    "   params):  # Additional configuration, Any params passed to the Estimator constructor are in turn passed on to the model_fn\n",
    "  \n",
    "  model = MyModel(feature_columns=params['feature_columns'])\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.PREDICT: \n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions={'predicted_rankings': model(features)})\n",
    "  \n",
    "  with tf.GradientTape() as tape:  \n",
    "    predicted_rankings = model(features)\n",
    "    loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "    loss = tf.sqrt(loss_obj(labels, predicted_rankings))\n",
    "    # loss = tf.sqrt(tf.reduce_mean(tf.square(predicted_rankings - tf.cast(labels, tf.float32))))\n",
    "\n",
    "  # | Evaluation Metrics\n",
    "  rmse_obj = tf.metrics.RootMeanSquaredError(name='rmse_obj')\n",
    "  rmse = rmse_obj.update_state(y_true=labels, y_pred=predicted_rankings)\n",
    "  \n",
    "  mae_obj = tf.metrics.MeanAbsoluteError(name='mae_obj')\n",
    "  mae = mae_obj.update_state(y_true=labels, y_pred=predicted_rankings)\n",
    "  \n",
    "  eval_metric_ops = {'rmse': rmse_obj, 'mae': mae_obj}\n",
    "  \n",
    "  # | Tensorboars\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  tf.summary.scalar('mae', mae_obj.result())\n",
    "  tf.summary.scalar('rmse', rmse_obj.result())\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    gradients = tape.gradient(loss, model.embedding_layer.trainable_variables)\n",
    "    optimizer_obj = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    optimizer_obj.iterations = tf.compat.v1.train.get_or_create_global_step()\n",
    "    train_op = optimizer_obj.apply_gradients(zip(gradients, model.embedding_layer.trainable_variables))\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predicted_rankings,\n",
    "                                      loss=loss,\n",
    "                                      train_op=train_op,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=loss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "model = tf.estimator.Estimator(\n",
    "    model_dir='model_dir',\n",
    "    model_fn=my_model_fn,\n",
    "    params={\n",
    "        'feature_columns': feature_columns\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvZskSAdam5e"
   },
   "outputs": [],
   "source": [
    "model.train(input_fn=lambda: input_fn(cleaned_ratings_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LP72MHw9Pw-"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viK4Ma7tKLzs"
   },
   "outputs": [],
   "source": [
    "!kill 1893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IN26aodqWQve"
   },
   "outputs": [],
   "source": [
    "model.evaluate(input_fn=lambda: input_fn(cleaned_ratings_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RV-q_YR2l667"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(input_fn=lambda: predict_input_fn(cleaned_ratings_path).take(10))\n",
    "\n",
    "list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaEz8f9OnXBW"
   },
   "outputs": [],
   "source": [
    "rm -rf model_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "SSvEmGRFGaqS",
    "outputId": "fae427c6-dc80-4a4e-b2b8-3028944399fc"
   },
   "outputs": [],
   "source": [
    "ls model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjVgf3n5HDxR"
   },
   "outputs": [],
   "source": [
    "rm graph*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF2-Estimator-Example.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
